1: 的时间。
1: 啊，我是来自于 Nasty 我叫刘晓国，啊，目前是负责中国的 Nasty 在中国的社区。
2: 啊，今天我给大家下一个业务，好像变窄了这个。
1: 今天我给大大家，这个不是第一页吧？
1: 今天我给大家讲的这个议题呢，是啊运用 Elastic，Elastic 向量搜索创建 RAG 应用。
1: 我不知道大家有对在这里有多少人对 Elastic 有了解的，还是听过 ES 的，举一下手。
1: 其实，哎，其实蛮多的，我就超出我的印象。
1: 因为我们在啊27号在深，呃，腾讯大厦有一个线下的分享，半天啊，大概有200多人报名。
1: 所以说今天我们讲了一个像这个话题呢，其实可能在在座的有很多人也挺感兴趣的，就是你拿数据，如果大家不不了解的话是可以知道，就是抖音啊、滴滴啊、美团啊、携程，但你们的搜索那个引擎。
1: 推荐引擎其实也是用伊拉斯去啊创建的，就这个呢给大家，啊，目前是世界上最呃最大的那个搜索引擎，啊，主要是在购物业务里面，比如携程啊。
1: 找工作像 CSDN 大家如果用的话，里面搜索引擎也是来自于 Yandex 啊，然后呢，这个呢，是我的这个一个博客，然后大概有两，收集了2,000多篇文章。
1: 这边呃如果是大家，包括最最基础的到最高高高端的这个这个这个这个文章都在这里，基本上有每天都会有更新啊，大家可以看一下。
1: 然后为什么要啊做这个，向量这个就是我们可以看一下这个就是智能时代的搜索需求，就是之前我们大多搜索主要是对词汇搜索需要，比方说我们用 Ctrl f 啊，如果搜索一个词。
1: 然后就是如果这个词在这当前的这个浏览器里面没有的话，它是找不到任何结果的。
1: 那相反，如果是说我们啊如果是说做向量搜索的话，我们可以说呃比方说我我爱你啊，我喜欢你，那这个两个字它也可以，比如我喜欢你的话，我可能会找到我爱你的这个文章。
1: 或者是 i like you，i love you。
1: 如果是多语言模型创建的向量，它实际上是可以搜索到的。
1: 所以说这这个大家你可能看到就是 ChatGPT 可能是很多是人，呃，就是我们通过语音进行一个查询，然后可以得到一个搜索，得到我们想要的结果。
1: 就是说这个过去呃需求的话，我们大多数可能是存在就是全文搜索啊，结构化的一些搜索。
1: 排优是分词，然后呢我们对未来的需求的话，可能是啊对这样一个向量搜索的一个需求。
1: 我们简单一个例子可以看一下，这个就是假如说有个有个句子，how to set up in a search m l 就 m l 其实是 machine learning 啊，是机器学习。
1: 它甚至可能会跟我们的一个公司叫 printer 这个公司，啊，最早使用 in a search 创建这个机器学习的，这样我们我们收购它了。
1: 所以说如果是我们搜搜索 printer 甚至可以找到积极学习，in a search 然后 setup 比方说是，它可能是安装的意思，或者 configuration 的意思。
1: 啊，所以说这个词呢，就是一些联联想啊，就是说，那么比方说这样一个结果，我们是通过词汇搜索的结果，你可以看一下比如说 how to set up，in a searchy 呃，m l 其实第一个结果不是很相关的。
1: 但是你可以看看第一个词，上面是不是有一个有个叫做一个 up 你看到没？
1: up to four g 对吧？
1: 它 up 和 set up 它不在一起。
1: 但是它不能达到这个 set up 的意思，所以说虽然你这个词可能现在在另外一个地方出现了，但是它俩不在一起，它是不不是一个真正的语义的搜索，虽然你这个词是甚至就是说我们有没有遇到这样的情况。
1: 比如说我不想吃冰淇淋，但是我，它可能一个句子里面说我吃冰淇淋，它找到这个句子的这样一个一个文档，但是它真正不是我们想要的，因为跟我们的意思是相反的。
1: 虽然它有百分之好几十的匹配，但是那个不，它是真的是我们不想要的。
1: 啊，就我不想吃冰淇淋，但是它真是就是我，那假如有文档里面有我吃冰淇淋，它就可能会搜到这个文档，但是它可能排名还是靠的很前，但是它并不是我们所想要的。
1: 那语音搜索其实它是克服了这样一个优，一个缺点。
1: 那么语音搜索的结果，比方说我们通过语音搜索的结果，我们是 how to set up in a social machine learning 啊，那第一个呢，其实它的结构是非常相关的，就是 machine learning 啊 settings 对吧？
1: set up machine learning features。
1: 那这些文章是非常和我们相关的，那么就是这个就是和这个词汇搜索的啊这样一个区别。
1: 然后其实在现在的时代里面，我们会甚至搜索图片，比如说我们现在一个人穿了很范性的一个衣服。
1: 然后昨天说，哎呀，我们喜欢那个呃那个裙子，或者是那双鞋，那么就是我们可以通过一个文字，比方说在这个地方我们可能呃覆盖呃雪的山，它就是文字，我们就可以搜索到这样一些图片。
1: 对图片进行搜索。
1: 这个有一个非常的一个一个一，这这下面一个连接啊，有一个非常详细的这样一个一个例子。
1: 然后比方说我们甚至通过头像比对，比方说一科目四网站的话，我们甚至把我们的图片发到网上一起传一下。
1: 发现哪些店在卖些这个衣服或者鞋子相似的，那么也可以找到。
1: 然后我们看看以那时期的啊向量搜索及最新进展，在以那时期在这个向量搜索方面有两两类的向量，一类是两类向，一个叫做 dense Vector 就是密集向量。
1: 另外一个叫做 sparse Vector 啊，这个稀疏向量。
1: b 级向量的话，它就是把这个文字、图片或者声音，啊，它全部变成一个数字。
1: 它全变成数字，那个向量是怎么形成的，也可能是通过神经网络，啊，或者转接，对图像来讲会转接神经网络进行形成的。
1: 那么就是说，甚至我们经常会描述一个人，比方这个人，啊，虽说比方说一个人，我想找一个找个心仪的一个人，比方说我们说他身高身高是1.6那么1.6可能1.7 1.9可能太高了。
1: 我就不很喜欢了。
1: 然后比方说1.65啊，你的分就比较高一点，它全是数字化的，比方说头发黑色的，啊，多黑，是的，它全是数字化的，那么就是多黑的是自己最满意的一个一个一个素质，比方说学历。
1: 但是呃硕士呢，那可能最好的就是五五分，然后就是博士呢，我可能不需要博士，我就去给给个3分二分。
1: 啊，那个那个，可能就是完全数字化的，比方说身高，啊，皮肤颜色，皮肤深浅，大眼睛还是小眼睛，全部数字化。
1: 那当时我们要把这个整个的这整个向量形成个向量进行搜索的时候，如果我们把所有的数据都输入到啊向量数据库里面，其实你得到一个搜索结果是最相似的一个，而不是说我们传统的搜索。
1: 啊，比方说一个大于多少年龄啊，小于多少年龄啊，然后把一些很多东西甚至把它过滤掉了，但是如果是多维向量搜索，它是通过一个向量经多维的，多个维度的同时进行搜索。
1: 它可以对你和你的需求。
1: 他有可能不是那么身高完全合适你的，但是其他的，什么啊其他的这个学历啊，然后呃远近啊，啊，他这个完全是符合你的要求，他可能跟你最相近的一个。
1: 这个就是一个向量搜索的最基本的一个一个原理。
1: 另外一个刚才我们看到这上面就有一个稀疏向量，稀疏向量呢其实它这个呢有点像是，就是我们不需要做任何的这个啊 fine tune 也就是不需要微调，它是对这个传统的这个。
1: 它是算是通过一个文字的这个扩充。
1: 文字的这个扩充，比方说我们看到呃 volcano 对吧？
1: 呃，can canoes 对吧？
1: 然后它可能跟的是船只有关系。
1: 比方说 hire 跟着 rent 就是租借租赁有关系。
1: 然后那个 can know 也可能 boot 有关系，然后它这一个词可能扩充到好多个词，可能有20个词，然后它会形成一个一个一个一个一个整个的一个词汇的拓展，然后我们再进行传统的那个 BM 25的那个词汇搜索，它也可以做到。
1: 呃，做到那个语音搜索的效果，而且这个效果它的速度远远比这个呃 dense Vector 甚至它的速度更快。
1: 这个呢，它的它的效率啊，然后是非常高的啊，这个就是文本扩展的一个，扩展的这样一个例子，比如说 joy is you are looking for 然后你可以看一下它这个里面，比如说 joy 的有可能跟 Robot 有关系。
1: Android 比方说 looking 它是同一个词扩展，啊，joy 的有可能跟 galaxy 有关系啊，然后它这个拓展了过后，然后比方说你搜索的时候，比如 do Android dream of electricity 它也会对每个词进行拓展。
1: 拓展了过后，比方看它这之前有一些 match 啊，有一些 match 的话，然后它这个它这个拓展过后，它实际上有个分数的，它这个每个在它的，相当于是一个一个机器学，一个一个统计的一个概念。
1: 还是一个学习的概念，然后它把它，然后真正得的分数，它就是这样做出来的。
1: 实际上它是这个比那个密集向量啊它的速度会更快一些，啊，更快更有效。
1: 啊，我们以拉数据其实自己有一个叫 L3模型，包括我知道阿里也有一个他们自己的这个，啊，这个这个主要是对文本起作用的。
1: 然后我们是看看这个向量搜索的原理，我们可以看一下，就是说我们首先把这个所有的文档，图像啊，啊，或者声音，或者是啊 video 或者等等，都是文档，我通过这个通过这个，对不起啊。
1: 对，然后就可通过这个 transforming to 这个 inviting 我们通过嵌入模型把它变成嵌入呃向量，存入到这个银行分类数据库里面。
1: 当我们搜索的时候，我们使用同样的这个嵌入模型。
1: 把这个我们提出的问题变成那个向量，跟这个一样的操作向量，然后以那设计其实就是一个啊搜索引擎，它找出向量，在向量空间里面两个最相近的，或者是最相近的五个或者十个。
1: 这样一个排名的这样一个向量，然后它就是我们所需要的结果。
1: 这个就是密集向量啊，它的工作原理。
1: 那个这个其实它是非常简单，然后它其实它分布的有有几个步骤，第一，比方说我们监测一个建立一个这个啊一个 embedding model 我们传传入到机器学习节点，然后创建一个 inference API 这个 inference API 可以跟我们调用。
1: 比如我们的数据调用这个 inference API 我们可以让它形成存入到一个隐藏社区里面，形成一个缩影。
1: 当我们搜索的时候，也会调用这个 inference API 然后去做这个 k n search 就是最最近的最近邻的这个搜索，然后得到我们所需要的结果。
1: 这个其实它的原理是非常简单的，如果这种模型的话，呃，一般的是对，这个不是说图像搜索，是因为这个啊这个传入的模型在继续学习，目前的话我们一般，如果是头像做成这个向量的话。
1: 我们一般通过这个呃 Python 代码或者实现，然后把它写入到你那个集群里面去存储。
1: 继续学习的这个，虽然这个原理就是说我们通过 Innot 的这个工具。
1: 从哈根费斯也好，或者是啊这样一些网站里面，把这个模型传入到这个 in a search 里面去。
1: 啊，它得继续学习节点。
1: 然后呢，我们会去创建这个 inference processor 一个 pipeline 然后去写入文档，把这个文档的话，直接写入进去。
1: 如果你可以看一下这个的 DC inviting 虽然就是一个速度。
1: 那一般一个速度的话，就是一，实际上向量你简单的讲就是一个速度，全是数字的啊，全是数字的一个速度。
1: 那我们进行查询的时候，实际上哎，这个地方你可以看一下，就是说我们当然这个里面有两种查询啊，一个是传统的查询，另外一个是天阴的查询，这就是向量查询，向量查询我们把向量填入到这里面去。
1: 对这个对这个数，呃，对这个缩影进行查询的话，它会得到我们最相近的这些结果并返回来。
1: 所以说这里面如果如果是是如果是这样子讲的话，一个需要两步做，第一步的话，先要把这个查询的这个 summer clothes 计算出向量填到这里面去。
1: 然后再调用这个做第二次查询。
1: 但是我们啊也简化了，到8.7以后的话，实际上就一句话就行了，就是说啊这个里面都是向量的这个缩影，然后我们把加入这个 carry Vector builder 然后直接把这个 model ID 丢进去它就可以进行搜索。
1: 然后现在看起来还是很复杂，之后你可以看一下，它越来越简单啊，这个是早期的这些啊版本的。
1: 然后另外的就是什么搜索，你那设计其实向量搜索其实特别是密集向量，它的解释性特别差，因为你根本不知道它就是神经网络啊生成一个向量，然后就问它为什么会相关。
1: 其实解释性非常差，不像传统的呃收费搜索，它这个里面包含还是不包含。
1: 其实这个呢，就是我们使用，啊，我们甚至可以结合 BM 25就传统的搜索，我们的搜索，和这个稀疏向量的搜索，或者密集向量的搜索，我们把这几个都融合在一起。
1: 通过一个混合的一个打分，因为这个每个的啊这个向量的搜索，我们可以说它的搜索相关性是0%到100%啊，0.1零点几、0.99 0.88这个相似性的分数。
1: 但是这个传统的 BM 二五的打的分数可能是一，可能有20它的分数是不一样的，所以说我们可以通过一个 RF 的这个方法把它这个统一起来。
1: 然后经过一个综合的打分，这样可以提高召回率，也可以提高那个相关性，这个就是呃非常好的一个一个结果，实际上是比现在都是那些传统，纯做向量搜索的它效果要好很多。
1: 其实也就是说把这个 traditional 的这个 turn based score 和这个啊这个 Vector 的这个分数，然后结合起来。
1: 当然也有两种方法结合起来，但刚才我们看到的一种是 RF 另外一种就是这种叫 Convex combination 其实我们通常，比方说我们对我们那个应用领域，我们对这个传统的比较认认同。
1: 我们甚至可以给他个加权是0.9比如说这个是我不太认同，啊，项目搜索我们可以给他啊0.1的。
1: 但是这个是对你的应用场景你特别了解的情况下，你可能精准的控制，啊，那这个可以得到非常好的一个一个相关性。
1: 那么这个它的就是简单的分数就这么计算的，但是另外一个 RF 的它是不一样的，就是啊是这样一个公式，这个公式看起来很烦，很很复杂，其实很简单，这个 k 啊，你指定一个一个参数，比如是0。
1: 然后呢，他其实完全是根据他的排名，他完全忽略他的分数，就是 BM 二五的，他 a 这个这个呃搜搜索是排名第一的，b 是排名第二的，然后在另外 b 级项目搜索。
1: 他是 b 是排一的，c 是 c 文档是排第二的，然后通过这个这个如果是两路召回的话，通过这个工具就可以算出它的分数。
1: 它是不是简单粗暴，但是它非常有用。
1: 但是我这这这里这个公式只有两路啊，两路，它也就就是这个公式，如果是有三路的话，那么加这个公式啊，这样就可以很容易就把它算出来。
1: 所以说很多人啊甚至你可以自己拿个笔一算就出来了，这个就是。
1: 然后另外一个相似性阈值的话，这个也是很重要一个概念，比方说我们做相似性搜索的时候。
1: 特别是有些相似性，比方说一般，啊，比方说相似性百分之80%以下的，我们就不算，不要返回给我了，因为我认为就不相似了。
1: 比方说，特别是呃大家如果是做过像那个亲子鉴定，像那个呃 DNA 的那个鉴定。
1: 比方说95%相似，那这个肯定是假的。
1: 那95%以上的，我都认为可能是真的。
1: 所以说这个阈值是你自己啊算出来的，或者是你的心中的阈值。
1: 所以这个在实际的应用中也有很多用处，比方说啊我想呃特别是聊天与聊天的那种知识库，呃，因为有的结果的话，你通过可能像那个啊大模型去问它的结果，但是如果你问的问题我之前人人的问的问题，比如说我已经存到我的数据库里面了。如果是问
1: 的问题和之前的是一模一样的，或者相似度达到99%那我就不用去访问那个大模型了，我直接从本地的呃数据库捞一个结果，直接返回去就行了，而且很快。
1: 但如果是说它的相似度只有80%那我可能认为，哎，那可能这是个新问题，从来就没人问过，然后再做一遍，然后可能会大模型啊，再去查询啊，然后那个他的啊他这个就是可以去做很多一些东西。
1: 然后啊 in a search 目前它是提供了几乎啊我们想要的所有的一些功能。
1: 你比方说是，大多数的一些向量数据库，可能它只能存储搜索向量，它不能创建一个代理。
1: 刚才我们看到了，你那数据可以从第一学习节点，把你的那个嵌入模型传入进去，它可以帮你生成隐盖点的嵌入。
1: 啊，然后以那社区除了这个呃存储向量和搜索向量创建模型外，他，而且他有很多传统的这个啊这个文文本搜索，啊，就是所以说这个呢就是非常多的一些功能在这里，包括位置啊等等设计。
1: 包括啊这些 ESQL 大家这个也是啊，也也拿出也支持像 SQL 那种查询啊，就是一个，大家对于 SQL 比较熟的话，那么这个，所以这个呢是引拉设计非常重要，而且引拉设计是一个呃包括这个 CUDA。
1: CUDA 就是那个 GPU 的那种 graph 的这个这个搜索啊，就是说我们是原生的这样一个啊在 Lucene 里面建立的这样一个向量搜索引擎。
1: 然后我们在呃 Java 程序里面，其实在近期的一些版本里面，其实加了很多一些东西，特别是 GPU 的呃指令加速，因为很多，因为 Java 程序是做呃是 Java 开发的，很多人认为只有 C++可以做那个运行加速。
1: 其实在 Java 二点二二二零的时候 SDK 20的话，它就是可以做这个 SMD 的这种啊一一个指令多个数据的这样一个应用加速，包括 GPU 的一些加速，所以它这个性能会得到非常大一些改改变。
1: 然后另外我们提看到这个标量量化，我们知道向量是非常耗内存的。
1: 就是一个向量是四个字节，啊，比方比方说是 Openai 是1,563维的数据，那么它是非常，它这个数据再乘一个4，那么它的这一个一个一个一个一个 chunk 可能就是要占这么多内存。
1: 所以说以那数据在在是，在这个，而且你的图一般，特别是 HNSW 的搜索，它这个图车上是在内存里面的。
1: 所以以那数据做了很多些标量量化，就是把把一个一个复点所压缩的一个一个一个字节，它省了呃百分之呃75的内存。
1: 然后甚至可以压缩到四个字节，然后最最呃最疯狂的我们是做到呃一个 bit 就是3 32位我们压缩的就是一个 bit 很多人就是不可能不可能就是一个 bit 你怎么能呃32位怎么弄到一个 bit 其实这个呢就是待。
1: 会我们啊这个地方我们是有是不是有一个啊就是这个呢，就是我们进入南阳理工大学的最最新的一个技术，啊，开发出来 BBQ 啊，它是他现在已经默认的就是文字搜索。
1: 我们用 BBQ 技术，实际上他节省了95%的内存，而且他效果非常好，所以我们把它作为一个默认的这个文字搜索的，这个在全球是没有的，其他任何一个厂家是没有这个这个技术的，然后就是说我们啊也是在首次推出来了。
1: 然后这个呢，大家可以看一下这个单个查询并发，以前我们早期的以那设计就是一个下的一个 THREAD 现在我们每个 segment。
1: 有一个 THREAD 甚至我们可以做逻辑分，逻辑 segment 也就是说一个大的 segment 可能有多个 THREAD 啊，并发间的协同，那么查询的时候，互相这个 segment 之间查询是互相通信的。
1: 比方说，我查的比例远远的比例的距离更短，那你就不需要查了，你就直接退出，它可以节省很多一些资源和时间啊，这个就是很多啊这个里面其实我们讲到一些，其中有一些啊已经在之前讲过了。
1: 比方说并发间的协调啊，IO 并发，比如说一个查询可以访问多个多个 IO 以前是不可以做到的，特别是在鲁森10啊，鲁森10的话，啊，就是有那些酒的话，做了非常多的改进，它主要是做 g 链加速啊。
1: 比方说这个 IO 这个一个一个查询可以多个 IO 同时，呃，同时呃同时查询，然后稀疏索引对某个字段可能进行排，呃，可以排序。
1: 做优化，然后这样你查询的话，比如说我查最大值最小值，我看他不在这个范围，直接跳过去这一个 segment 啊，提前终止，快速模式，这些都是啊提前终止，比如说我一直在查询，一直查询。
1: 在循环很多次，也就是那个距离，只有那么很小很小的那种，把那个结果没有什么特别的改动，我们直接把它终止了。
1: 快速模式，这，所以说这些东西都是我们做的，非常多一些，包括逻辑分区，对，逻辑分区就是大的 segment 我们可能分两个到三个 THREAD 去完成。
1: 这个里面有很多一些描述啊，大家可以看一下。
1: 然后引导设计在这个啊其实我们最关注的就是呃你这个搜索呢，有几个阶段，比如说文文本词汇搜索，texture search 然后 Vector search。
1: 然后呢一个 Hybrid search，Hybrid search 其实就是混合搜索，我们把这个文本和这个向量搜索，然后混合起来，然后得到更好的召回率。
1: 然后呢，这个呢其实啊是非常好的一个，啊，比纯像的搜索要好很多。
1: 然后另外一个就是我们做 random intuning 特别是大多数需要啊这个大模型嘛，我们需要做 rag。
1: 如果你的相关性不是很好的话，你发个大模型，大模型产生的结果是个，不是你想要的结果。
1: 这个呢就是一定要做一个，那么这个呢就是啊这个呢我们更关注重新排序，重新排序的话，这个里面比如说重新排序，你可以看一下，这地方我们第一级的话，我们可能会有搜到100K 到一个一一百万的一个文档。
1: 通过这个 BMR 5啊，或者是 AN 的这个密集项的搜索或者 Spass Vector 的搜索，然后我们可以通过这个 MID edge 我们其实有个叫做 learning to rank 其实在我们搜索时。
1: 我们甚至可以通过自己训练的模型，这个模型实际上是是你自己啊训练出来的，它可以对我们的搜索结果进行打分，啊，甚至重新排重新排序，就通过银行设计节点我们搜索的时候。
1: 然后这个最后一个阶段，我们甚至可以用这个 AI BASED model 比方说通过啊一些大的厂商，它有这个重新啊排序的这样一些接口。
1: 我们可以重新调型，啊，甚至可以，因为这个计算分数，如果是另外一种计算分数，就是弄弄弄代码，啊，弄脚本机计算它的这个。
1: 绝对值，那么这样是更精确的，更精确的话，那么我们因为这个文档很少，它可以用这种方法做。
1: 如果像这些文档很多，你能用脚本去计算，那个速度是非常慢的。
1: 其实这些呢，我们都是在我们这个这个我们看一下叫 learning to rank 实际上就是一个一个一个表格啊，这个表格的话，它实际上相当于你自己啊建了个 Excel 表的话，像这种模式，然后你会告诉他这个结果和这个结果是否相关。
1: 如果相关，你这打个一，然后看的上有个一。
1: 如果是不相关就打个0。
1: 然后这个模型的话，当你这个数据如果是啊啊打，这个数据得到了过后，你可以通过 XG boss 的这个工具呢去训练这个模，得到个模型，这个模型可以上传到啊积极学习的节点。
1: 你上传上去了过后，我们在搜索的时候，我们甚至可以通过 rescore 啊，通过这里训练这个模型，比如说跟你的业务相关的。
1: 那么我们可以得到它的分数的这样一个重新排序。
1: 这个呢，就是有一篇很好的文章，大家可以去啊详细阅读。
1: 那么就是说我们做 rank 的话，其实有很多方法，第一种方法就是像调用这个 copair 或者是阿里巴巴或者是腾讯啊，啊，等等一些啊一些东西，因为他们有很强的这个机器学习的节点。
1: 啊，机器学习的像 GPU 啊，或者是各种啊很强的计算能力。
1: 我们直接调用这样一个节点，这个节点了过后，啊，你得到过后，我们可以直接去通过它去做，把你的结果调给它，然后通过大模型的这个 AI 的能力去重新进行排序。
1: 另外一个呢，就是说这个是可以多，经过多级排序，比如哈根费斯，我们让他排个，哈根费斯提供这个 rebalance 重新排序的。
1: 然后 Elastic 的 rebalance 它也是有一个我们自己的自自带的这样一个 rebalance 然后比如说 Cohere 啊，它有个这个厂商也提供一个 rebalance 的这样一个一个端口，我们把我们的结果提交给他，然后他会得到一个更加相关的、更加有序的这样一个一个结果。然
1: 后呢我们啊另外一个就是一个检索器，检索器其实这个呢就是对于多级的，比方说啊比方说这个 retriever 的一个我们混合啊多个的，包括这 RF 也就是我们推荐的这种方式啊。
1: 也就是你可以看一下啊 retrievers 可以有 KN 的，也有啊传统的这个传统的这个词汇搜索，呃，然后像搜索，包括这些呃 match 这些，它可以去帮我们做成多级的，多级的这种搜索形成一个一个一个 pipeline。
1: 然后我们可以看一下，另外一个就是我想讲到的就是一个呃一本书，大家说一本书很长，一本书比如800页的书，我怎么能讲，特别是讲 Openai 里面1,563个呃 TOKEN。
1: 或者这样一个围的，这样一个向量，苏总，他能代表整个小小说吗？
1: 他不可以。
1: 因为你这么小的短短的数据，所以说一定要把这个文章一般都要把它切成分段，就把它切成一个小片，把这个小片再做的向量化，一个小片一个小片向量向量化，这个是啊常用的技术。
1: 所以当我们啊做切片的时候，其实也有一些呃原则，比方说你把一个唱的弄得很大。
1: 那么就有可能你总结的这个调，得出的结果呢，有可能呃就像我天性就是不是很，呃，就可能分散了，你给他的东西很多。
1: 它有可能叫大模型，你如果总结的话，它有可能会啊它可能多个重点，它有可能是。
1: 如果是，但是如果你细腻度的话，特别细的话，它也不能得到它的很大的精度。
1: 所以说这里其中有一个啊你们需要自己去啊调整一个一个合适大小。
1: 那么就是说既不要太粗，也不要太细。
1: 所以很多人有时候啊把所有东西都给大模型，大模型它就会总结出来，它不一定。
1: 大模型如果是你给它东西太杂乱了，它会得出的结果有可能会是不是你想要的结果。
1: 但你很细也不行，所以说这里面需要一个分分款策略。
1: 分款策略的话，我在以拉数据里面一种就是叫做按照句子进行分款。
1: 比方说，呃，呃，open 的1,563个呃字的话，它不能超过这个字，那么就是说我们有可能都比方说我上一个段和这下一个块的之间有个重叠的地方，我们可能会重叠啊一个一个一个句子或者两个句子。
1: 这样不不至于在中间一块把它给掐断了。
1: 然后呢，比方另外一个呢，就是按那个多少个字。
1: 多少个词，比如说我100个词是重复的，那么一般一般的句子可能没有100个词，那么就是说我们至少每个块和每个块之间呢，它有一个重叠的部分，这样你在让大模型或者搜索结果给大模型的时候。
1: 它实际上它的意思还是完整的，就是不是从中间，比如说我今天来到呃深圳，比如说我今天来就卡断了，就1,563就到到这位置了。
1: 啊，所以这个是不行的，所以说它是需要一个需要一个一个策略，这个策略的话你可以自己选。
1: 你看这个里面，就是啊我们有个字段叫 semantic text 这个字段，semantic text 字段的话，这是在最新的最近的几个版本里面才有的。
1: 它是既代表可以是系数向量，也可以是一级向量，我们不需要去区分它。
1: 我们不需要去做任何的区分，啊，就是说都是叫做存在性测试。
1: 然后我们定义，然后它会自动的帮我分段，它会自动的帮我分段，然后我们不需要去考虑。
1: 然后你可以看一下这里面的，就是它有一个重复的部分啊，你看这个字，这个段里面，这个你看，on or on orbit 你看 or on orbit 就是它一个 CHUNK1和 CHUNK2它是有重复的。
1: 所以说这个啊是，当然这个重复的有可能是啊多少个字，第二个呢有可能是多少个句子，啊，这个他是帮我们自己做好了。
1: 所以就是说这个，你看8.16的话，默认之前是进入句子的。
1: 然后801，8801之前是进入单词的，801的话是进入句子的，但是你可能是一个句子还是两个句子啊。
1: 然后呢这个呢就是它会自动帮我们 chunk 然后分成一个一个分块，然后我们做这个然后把它就做成一个项链画。
1: 这个呢，它的搜索之前我们是不是特别特别麻烦，特别看的特别呃特别繁琐，你看之后的话你就特别简单，比方说定一个端点叫 spasby 然后你是按照这个 else。
1: 或者是 Openai 等等，或者是或者是呃任何一个厂商，或者是嗯以纳斯设计，它就是传输传输到以纳斯设计节点的嗯那个节点的话叫以纳斯设计。
1: 然后这个地方就做这么一个东西，然后我们做，写入文档的，做一个 Mac 你看，作为门派定义这个字段叫 small detect 这个可以是 sparse Vector 也可以是 dense Vector 都没关系。
1: 然后我们写入文档就这么写，跟传统一样写。
1: 然后搜索只有啊就 query string attacks 它就直接就搜索到结果了。
1: 它是非常简单，它的语法非常直接啊。
1: 然后我们甚至更简单，就是说在8.8加以后的话，甚至我们可以通过啊 SQL。
1: 直接去搜索，它可以搜索到我们想要的结果。
1: 然后甚至我们可以使用 match 啊，我们我们传统的一些，大家用 DSL 做隐藏设计过后之前的话就是用 match 啊，就是我们现在的搜索，所以说这个非常简单啊。
1: 然后以那搜集 AI 呢，它集成了很多的第三方的服务，比方说啊阿里云啊，就是呃 n n Thorp pick，the cloud，Cohere，conference 包括啊谷歌等等，这所有的我们都已经把它集成了。
1: 比方说你有已经有这样一个啊一个服务，或生成向量，然后进行搜索，我们都给，包括 review 我们都啊包括 Chat company 对吧，然后这个呃大语言模型的接口，我们都可以把它继承了。
1: 我们都有现有的这个，所有的这个地方。
1: 然后另外我们讲程三会议，以那设计是做这个，特别在 serverless 里面，我们把所有的文档都节省到对象存储里头。
1: 对象存储里面，所以一个就是一个 injest data 我们放到这个呃输入节点，然后搜索也是一个搜搜索一个一个 TIER 对吧这个一个群。
1: 把存储和分离的，然后所有的数据是存在这个对象存储里面，它几乎是可以不需要很少量的硬盘在每个节点，那么就是说但它的它的实时性啊是非常好的，所以说这个呢对于很多的应用场景会节省很多的成本啊。
1: 耳机的实现原理我觉得大家可能也知道，就是说啊我们如果数据不是很好，我们特别是呃 deep sick 比如说一问，说今年315有哪些产，有哪些厂商啊，哪些商家，被被提名了。
1: 他可能回答不出来，因为这个数据不在他，他在睡眠之前的话，他没这个数据。
1: 所以说它会产生幻觉啊。
1: 然后有三种方式可以啊提高这个精确性。
1: 比方说 Pre train 我们大多数可能没有人有这个能力去创建一个大模型，可能 deep seek 有这种能力。
1: 第二个从 fine tune 我们微调。
1: 比如说在医药、法律这专用的领域，我们找数据科学家呀，继续学习专家，然后做成这样的。
1: 但是即便是这样的话，有时候你的结果不一定是你想要的，如果数据集不是很大，比如说青霉素可以治感冒。
1: 我突然就醒了，我说调，微调一下，我说青霉素可以治感冒，结果他根本，你第一天第一次他，可能他能还起作用，第二次他就不一定知道，你一条数据影响不了他，啊，青霉素可以治感冒，这这这这是胡扯，对吧？
1: 所以说他这有些东西，你的数据集不能达到一定的规模，他并不一定，它影响它的大模型的啊它的这个这个结果啊，就是说这个情景学习，其实最最避免出现幻觉的就是通过 REG。
1: 所以说我们可以看一下这个 RAG 的实现原理就是很简单，就是用户的问题，啊，代码层的一片调用，我们可以通过将，通过使用这个 sematic 就是啊语音搜索或者是词汇搜索，多路召回，你当然可能有多路。
1: 功能找回过后，到这个地方，你看这右边的，我们把自己的原有的用户的问题放在这儿来，把这里搜索的结果放到 contest window 里面，一起加个，一起叠加上，然后再给大模型，大模型再思考，再出结果，它就不会有幻觉了。
1: 对呀，他这个就是他是非常的非常的那个。
1: 啊，我们在银行收集一些案例，我给大家分享一下，就是说啊这个，你看就是我们传统的就是我直接问大模型啊，比如说 user 直接问大模型，比如说315，哎，哪些商家啊，收到的。
1: 啊，报被曝光了，他不知道，他根本就不知道，所以如果是把315的这些所有的数据都放到以那社区里面去，我们呃通过 Hyper 社区等等，然后再搜索到的结果，前五个结果都给大冒险。
1: 大冒险才会产生我们想要的结果。
1: 这个呢是一种非常好的一种方法，它可以避免幻觉。
1: 但另外一个就是建这个啊这个 red 然后我们可以看一下，就是 user 啊，给一个问题，然后给大模型，然后大模型去调用相应的工具，然后去查询不同的啊这个结果，或者再按照顺序去调用不同方向性也好。
1: 或干什么，然后最后得到一个结果，虽然你给他一个任务，比如说我要出行旅行计划。
1: 你安排一个旅行计划，然后呃从从北京到呃到新加坡，那有可能就是安排坐船呐，坐车呀，坐出租车呀，然后订酒店呐，他整个都帮我搞定了，对吧？
1: 这个就是一个其实这个就是一个很简单的一个，这个有一篇文章啊，就给我们大家。
1: 然后刚才前面那位讲啊，老师也讲过这个。
1: 如果大家你可以看一下，就是 deep seeker 他问呃 Elastic Stack 版本是多少，他说是8013，他其实是不对的啊。
1: 就是说我们最新的版本肯定不知道，但是 Openai 它就很很 smart 然后它就是说它知道是最新的版本，它就知道去搜索网络，它就找到这个，它能得到一个正确的答案。
1: 所以这地方它有个这样一个 search 这个这样一个东西，它会自动掉。
1: 实际上刚才也就是呃前一前一位老师也讲过了，就是它有个平半，就是说我们通过，如果是这种方法的话，你通过 RAG 的搜索的话，如果是你要通过一个 RAM 的形式判断是不是达到你的要求。
1: 如果是不是，如果不是达到你所有的要求的话，这个时候你可能会去找一些其他的工具去完成，啊，你所想要的结果。
1: 然后这个呢，就是我做的另外一个一个例子，也就是两年前的啊，这个例子的话，大家一个，就是我们给个任务，给大模型，我不是直接去访问我的自由数据库。
1: 然后再交交给大模型，我是直接交给大模型，我告诉他，大模型在这里就是 reasoning 它会去跟着我们的这个呃分析分析我们的这个原因，然后它会调用现有的一些工具去完成你想要的工作。
1: 这个呢这个大家有没有看到，这个挺有意思的，就是啊这个就是我这个例子啊，就是我有一个，这个实现的 UI 实际上是那个倩丽的去做的，是网页那样，啊，不是用那个，像有的 MCP 的那些客户单啊去做的。
1: 这个就网页版，你看，比如说我这个没有告诉他任何一个呃缩影，隐藏数据的缩影，我直接就是完全根本就不用告诉他，我直接在隐藏数据里面打印打了一个这个，就打的这个。
1: what is 不是，what is the city name？
1: for the cheapest flight 对吧？
1: 然后他就会告诉我是，啊，对，这个，我是从从中国到美国是最最便宜的是什么？
1: 然后他就说是这个272这个其实是 Kebana 如果大家用过 Kebana 里面一个一个 flight 一个一个例事例缩影啊。
1: 然后我用中文问，我说从中国到美国的最低价格是多少？
1: 然后他告诉我是272他回答是中文。
1: 大家如果是用过可以拉数据的，也可以放在你那个 flight 的那个那个事例啊，你使用我这个例子，他就完全解决，然后直接是从这个你直接是从这个资源语音方式进行查询。
1: 甚至你不需要懂 SQL 啊，ESQL 或者 DSL 等等。
1: 然后这个呢就是啊 MCD Server 实际上它就是通过一个啊一个标准的一个一个协议啊，然后我们通过讲 USB 一样把大家都串在一起了。
1: 这个呢，其实通过 cloud cloud 这个工具啊，啊，大家如果是啊对一个 Desktop 这个也是一个很完整的例子，它也可以实现我们通过资源语音方式去查询我们的银行数据，这样一些数据。
1: 一些缩影，然后甚至帮他们去，呃，你问他一些什么，其实我有一个有个有个视频，大概呃两个小时了吧，然后那个呃有个人讲的非常详细，在我的那个 b 站的视频里头，大家可以看一下。
1: 另外一个就是提高查询的话，我们可以看到另外一个结果，就是啊有些方法，比方说 tell me about a fish 大家我们看一下，我们搜索的时候，实际上是我们提出一个问题。
1: 说从从美国到中国到美国最便宜的机票，实际上它就是一个一句话，但是我们搜索的是整个文档，整个文档是什么？
1: 一个 chunk。
1: 是1,000 1,563个字的这个一个 chunk 或者是呃这多少一个字一个一个 chunk 我们是一句话去匹配一句我一个文档来的。
1: 另外一种方法我们就是把我们的问题把它进行拓展，也就是叫做 hypothetical 对假想的这样 document embedding。
1: 实际上我们是，他是告诉我有多长，比如说生成个1,563位的这样一个一个文字的描述，啊，告诉我是 fish 然后我们用这个这个文档去再匹配再匹配那个我们的 chance 生成的那个向量。
1: 那么它的召回率可能更好。
1: 这个效果很好，所以说这个呢，其实你可以甚至让它生成十个问题，啊，十个这样的东西，然后做一个平均值相加，然后再进行 match 但是效果有可能会提高。
1: 所以这篇呢有一篇文章，我这里面有一个啊这篇有一篇文章，就是说我们把我们的问题呢比方说 in js 中把它分成片，分成 chunk 的，分成分成块，分成块过后，我们把它关键词提出来。
1: 甚至我们让它也提出有这一这一个篇文章，这篇文章，呃，就是100 1,563个文字的这个 chunk 我让大大模型说，你生成呃5个、10个这样的问题，能提出问题的，这个问题关注这个题的，那么它生成这个，然后都做成向量。
1: 然后做新的向量的时候，然后把这个向量和这个原始的那个向量再进一个加权，这个加权的话，我们把它存到 ES 里面去。
1: 当我们提的问题的时候，我们把它生成这个呃就是 hyperstatic 也就是相应的这样一个把这问题生成，比如说1,563位的这样一个啊一个文档，然后生成的向量，然后进行匹配。
1: 然后我们这个地方可以看一下，这个有一个叫做 Hybrid scoring。
1: 实际上就是他做复合搜索，然后我们可以对这个呃提出的问题可以给这个 key phrase 或这个呃这些东西都可以进行一个，这个机关有一个就是这样一个一个原理，就是说他 Meta data 然后呃比方说通过他去做 NTP 比方说今天的会议 data far 什么什么什么公司。
1: 然后这个 document 然后去有 potential function 呃，你你的文档每一个 chunk 的话我都可以生成一个生成一个呃潜在的一个几个问题，把它生成向量，然后这个呢，key phrases 把它生成向量。
1: 然后把它整个的通过这样一个加权，比如说我认为第一个那个原始最重要，然后啊这个 NTT 可能是0.1 0.158的加权，然后新的向量再存入到数据库里面，然后我们在我们搜索的时候。
1: 我们可能把我们的问题变成一个 Hyde 的那样一个东西进行匹配的话，那么它的这些都是一些技巧啊，就是说它可以提高我们的一个啊搜索的一个一个进度或召回率。
1: 啊，今天呢，我因为我是在这个深圳，应该是在啊虽然4，呃，7月27号的下午的，呃，那个南南山那边，呃，腾讯大厦那边有一个一个一个线上的，一个线下的。
1: 所以说大家如果是有兴趣的话，可以进下这个群啊，到时候我们可以在群里发那个访客码，大家可以去参加一下。
2: 另外就是我们一个公众号。
2: 啊。
1: 今天我先讲课这里，因为讲的可能会内容比较多啊，如果是大家有什么问题可以提一下，谢谢大家。
3: 王老师，选一下第几位？
3: 刘老师，你看举手的同学，哪位先来？
