1: 有两位讲师，一位是呃马金龙老师，呃，一位是那个呃我们来自于字节字节跳动的蔡东怀老师。
1: 对，然后呃第一第一个半场的第一个来自于去呃趣玩科技的算法负责人马金龙老师，他给大家带来的课题是开篇情绪陪伴大模型的研发以及应用实践，好，那么我们掌声有请王老师。
2: 感谢王老师的介绍，对。
2: 很很有缘，都姓马。
2: 对，也感谢组委会的邀请，然后感谢大家周六，然后抽空过来参加我们这个技术研讨，对。
2: 我今天的分享题目的话是呃全开天，轻量级大模型在这个研发及实践的一个过程，对。
2: 呃，也基于前面老师介绍的，对，我们这个主题的话主要是锚定一个场景，然后做相关的一些模型的研发。
2: 啊，刚才有这个莫老师，包括罗老师也介绍了这个场景应用的一些知识，我觉得受益匪浅。
2: 对，好，那我们就长话短说，然后我大概介绍一下我，我本人的话是叫马金龙，目前是在趣网科技负责媒体算法这一块，主要的这个内容呢，主要是现在在 OIN AI 这一块的一些模块。
2: 包括大模型的文本方向、语音和图像这个方向。
2: 呃，今天分享题目的话，其实是有这四个大的模块，那最后一个讲的主要是做一个小小的总结。
2: 对，那第一块的话主要是社交领域的一个用户的诉求和场景，就是我刚才说到了我们铆定这个场景主要是社交领域，那么对社交领域的这个洞察和认知，保证我们后面这个路是走的对的。
2: 那第二块的话是，开元情感开放大模型的一个规划。
2: 呃，这个有了这个场景，我们怎么去造，怎么去执行？
2: 那第三块的话是整个这个分享的一个一个核心内容，主要是开元情感开放大模型的一个研发之路。
2: 然后最后一块呢，就是刚才说到了一个应用的探索和一个未来的展望及总结。
2: 那我们回到第一块关于社交领域的一个介绍。
2: 对，这是今年25年年初的时候一个呃国际消费电子展 CES 上面的一个截图，然后这个里面其实表达了就 AI 陪伴或者 AI 社交这一块的一个对象。
2: 那么呃左边这个图呢，其实是说明了 AI 陪伴的硬件产品在目前的一个社交领域其实是很风靡的。
2: 那么右边的这个数据呢，展示了无论是国内外还是欧美，对社交这个产品的一个认知和规划，对。
2: 那这一块的话，其实表达了就 AI 陪伴或者 AI 社交的这个产品，目前是在 CS 上面是一个主流产品。
2: 对。
2: 第二块的话是有了这个硬件的，那么是软件这一块，或者应用这一块也层出不穷。
2: 对，那这个关于软件的这个陪伴的话，其实有这么几家大的公司在做，啊，其中有一块刚才也是有这个豆包的老师来讲，那么豆包在 AI 陪伴这一块其实做的蛮好的。
2: 对，那中间这一块的话是逐梦岛，然后再再左一点的话是像他们的语音大模型，再到上面有做产品应用的，包括星点，包括 create c，create AI 就 c 点 AI 的一些产品。
2: 对，那这一块的话其实想表达一个意思就是除了硬件的这个分离以外，那软件的应用也层出不穷。
1: 对。
2: 接下来这一块的话，主要是想表达这个我们对刚才锚定这个场景的一个认知。
2: 对，这个认知的话也是参考了很多呃国内外的资料，然后有一个相对比较准确的一个表达。
2: 对，那呃对这个对象或者是我们的一个用户画像呢，其实它的每个点都有诉求不一样的。
2: 比如说我们呃对于这个下面这个表格里面有写到，对于对于年轻的一些儿童的话，那他的一个点主要就是刚才说到的一些毛绒玩具，然后一些益智类的一些产品。
2: 对，它的一个驱动因素的话，更多来自于这个当下的增长。
2: 那么市场的特征的话，主要是打造一些特定的这个王聚的 IP 然后形成一些核心竞争力，这个是这一类用户的一个诉求，或者用户的一个特点。
2: 那么第二部分的话就是我们呃铆定的这个 z 时代的这个用户，或者是面向青年的这个用户，他的一些特性。
2: 那么这一块的话主要是围绕 AI 情感的陪伴产品或者 AI 的一个筹码，那么它的一个驱动的话，也是受单价的一个增长影响，那么这一块的话，它可能更多的一些是社交或者情感陪伴的一个虚拟应用，或者是虚拟宠物，或者是一些商务助理。
2: 对，那最后一块的话是呃关于老年人的这一块的一些陪伴产品，啊，这一块的话主要国内外是围绕这个陪伴的一些机器人去展开的。
2: 对，那这一块的一个驱动因素呢，是因为老龄化的这个社会到来，然后老年的人口基数增长。
2: 带来的一个点。
2: 对，那它的市场特征的话，主要是更多的在老年人的一些隐私保护，以及医疗和情感的一些关怀上面对。
2: 上面这个这个分析的这个表呢，其实是呃想表达这个 AI 陪伴在年轻人的这个场景里面，它的一个功能诉求，主要是模拟虚拟男友和女友，这个占比的话已经超过20%了。
2: 有了前面对于这个整个用户群的一个呃这个诉求的一个洞察，那将来我们对于我们某一个时代或者青年人的这个全球画像，也参考了 Sora 这个研究院发了一些这个相关的一些研究报告。
2: 对，那整个在这时代的这个全球画像的话，其实是有着四个板块。
2: 其中一个板块的话，第一个板块的话，是对时代这个年轻人的一个精神追求。
2: 对，那么这里面有一个比较关系，是平和，带有快乐，带有人际关系，带有成就，带有意义。
2: 对，这里面有相关详细的资料的话，大家有兴趣可以去网上找一下，我们只是展示这个最终的结论。
2: 啊，第二块的话是 z 时代的一个精神来源，那这里面的话就涉及到兴趣关系和自我成长，对。
2: 啊，第三点的话是 z 时代的一个年轻人的一个精神充电桩。
2: 它主要是来自社交、独处和公共空间和音乐，最后一块的话是新时代的一个精神伴侣，来自于找搭子。
2: 对，这里面综合起来就看一下我们做的这个事情，它的一个呃群体画像，更多的是来自于兴趣、社交、找搭子、快乐。
2: 这么几个关键词组合到一起了，对。
2: 有了前面对于用户层，包括我们做了一个事情的一个基础认知，那接下来我们要看一下，就是这些群体他的一个呃精神诉求，因为我们是做一个社交的情感陪伴的。
2: 陪伴的一个大模型，那么他的一个精神诉求是什么样的？
2: 那么其中有一个点就是，这也是呃这个心理学公众号上的一些内容，就是孤独是这个时代的一些主旋律。
2: 那孤独的背后是一个复杂的一个情绪诉求，那么不同的情绪诉求，它的一个表达是不一样的。
2: 比如说开心，他更多的是希望别人得到别人的表扬，啊，害怕，更多的是呃为了回避或者是呃，自我保护，对。
2: 那么我们走到这一步，其实我们这个问题已经很清楚了，那么主要的这个问题就是这时代的年轻人，更多的忙于奔波或者忙于生活，那么他们在情感陪伴这一块的话，其实是满足度很低的。
2: 那么就会显得越来越孤独。
2: 那么就需要一框，这个，或者一一一个大模型，或者是一个完整的一个智能 agent 能够去解决他孤独的这个问题。
2: 对，所以基于这个点的话，我们公司也在这块做了一些探讨，然后专门去定义了一个一个主题，就是解决孤独。
2: 对，那解决的办法的话，就是我们今天这个整个这个，就分享的一个内容，就用 AI 的一个方式来满足用户的一个清仓投放的需要。
2: 对。
2: 这个里面还有一个就是，对，展示为什么不是用 keep sick 或者是呃 ChatGPT 或者是用呃其他的一些模型去解决问题，而非得自己去造一个模型。
2: 对，这里面有一个简单的展示。
2: 对，左边是我们自己做的模型，然后中间是业界主流的这个通用大模型。
2: 对，他在这个点上呢，突然问了一个问题，就是呃我我心情不好，失恋了怎么办？
2: 大家可以看一下。
2: 然后左边其实更多的是去在情感方面的一些满足，而右边的这个更多的是如何去解决问题，是一个逻辑或者是一个常识性的回答，对。
2: 从这个点可以看出，就通用的大模型，其实是在这个聊天或者是社交这个场景的话，不适配我们的这个诉求，对。
2: 那走到这一步，那我们就去自己能发一个自己的一个气象配方大模型了。
2: 前面一一小节主要是讲讲完我们对于这个呃这个造的这个事情的一个洞察，那接下来第二部分的话是我们要做一个青岛配方的一个规划。
2: 对，然后这规划内容的话主要有三块，第一块内容的话是关于这个呃大模型的一个定位，对，这个定定位的话主要聚焦这几个关键词。
2: 第一个的话是聚焦于实时的社交场景，对。
2: 然后提供智能运营语音或者是大模型的一个技术能力，对。
2: 然后提升用户的一个社交体验。
2: 这里面有有两个点，第一个点的话就智能语音，第二块的话是呃这个大模型的一个能力，那今天我们主要讲的是后面这部分的一个能力，对。
2: 有了这个定位以后，接下来我们看一下，就是我们做的这个呃这个大模型如何在垂直领域打造竞争力，就是我们的从几个方面来说明我们做了这个大模型，它比通用的大模型有更强的一些竞争力。
2: 对，呃，第一个点的话是它可以支持新的社交场景的，这个新的业务探索。
2: 比如说现在原生的一些 native 的 AI 社交产品，包括星野、呃，冰岛、小冰岛、包括张次元。
2: 包括搜觅的一些探索，对。
2: 那第二部分的话，它是主要是围绕这个呃复合用户的一个诉求，对，刚才也讲到其实我们对于用户这一块的了解相对比较深的，那在用户的这个情感表达里面，其实孤独是这个用户的一个主要的需求。
2: 那我们如何去用大模型的方法去解决这个孤独？
2: 第三块的话是呃关于大模型这块数据安全，那这一块的话包含训训练和推理的这个数据安全。
2: 因为有些这个场景，包括我们社交领域，用户对这个个人隐私其实要求比较高的，对，那么大家也不希望把这些数据做泄露，对。
2: 所以说需要一个自然的大模型去保证这个数据安全。
2: 对。
2: 最后一块的话是成本优势，对。
2: 无论是用通用的大模型，还是第三方的，都会面临这个高并发下面 TOKEN 的一个成本，对。
2: 所以通过自研能够降低这个成本。
2: 对。
2: 第三块是关于造大模型这个事情上呢，我们考虑的这个，可能我们团队的这个技术实力，包括这里不够，那我们选择了这个业界厂学院的一些方式，跟业界知名的一些做大模型的技术。
2: 这个学校，高校去合作，然后融合心理学的一些理论，来打造这个晴啊计算和相关的一些晴啊晴啊陪伴的大模型。
2: 对。
2: 第三块的话是关于，开元情感开放大模型的一个研发之路。
2: 对，这一块的话主要是讲，就基于我们的规划，如何去呃研发这个情感开放的一个大模型。
2: 这个说到大模型的研发呢，离不开这个大模型的 Samba 服，呃，模型的架构，包括数据，包括算力。
2: 对模型架构这一块的话，我们还是选择了这是我们那个传输框框架，对。
2: 这这一块补充一下，呃，早期我们是用了 dance 的这个成语模型，对。
2: 现在呃也逐步切换到 MV 的一个框架上面，对。
2: 第二块的话是数据这一块，这一块的话呃也是做大模型这个呃避免不了的这一块的核心工作，然后我们从数据的获取、筛选到清洗，达到构成我们整个这个呃数据集的一个指令，然后去提升这个模型的一个能力。
1: 对。
2: 这三块的话是这个模型的训练，呃，训练这一块的话主要是训练的这个 GPU 包括训练框架，包括我们刚才说到这个训练算法，以及我们在这一块提出了这个社交领域创新的一些，强化学习的办法，和提升情感陪伴的一些能力。
2: 那以下主要这个，刚才提到的三帮普，一个介绍。
2: 那首先介绍一下我们开店数据处理的一个 plan 对，这一块的话主要有五个点。
2: 第一个点的话是数据的采集，对。
2: 呃，然后这块的内容的话，主要是有有两个小点，一个是收集业界的一些社交领域的这个开放数据集。
2: 第二块的话是呃对情感的数据集进行一些改写。
2: 对，因为验证开朗的这个数据集很很很多是当龙的，或者是说呃是一个情感计算的，对，它不是一个社交的，多轮对话的一个数据集。
2: 对，然后第二部分的话是数据的预处理。
2: 对，预处理这一块的话，刚才也说到对，很多数据集是单轮的，我们要去做这个多轮的改写。
2: 对，那改写的过程中就会面临很多问题，比如说数据的连贯性，包括这个角色的一致性，包括这个呃问答的一个准确性，对，各个方面。
2: 对，那么第三块的话是数据的弱标注，呃，这一块的标注的话主要是经过了呃机器的，对，规则过滤的一些方方式方法。
2: 对，这个在昨天有很多老师也讲到了关于这个数据机机器过滤或者是机器提取抽取的一些办法，对，我们采取的方法也差不多，也是呃通过一些简单的过规则过滤，把一些明显有问题的一些数据剔除掉。
2: 然后逐步形成一些能够符合我们这个场景使用的一些数据集。
1: 对。
2: 呃，然后第四步的话是一个数据的一个精标的一个过程，对。
2: 因为呃有了弱标注的这个数据其实是没有办法去直接拿拿回来做训练的。
2: 那我们就要需要去做一个金标的过程，那金标的这个过程的话，其实也是分人跟机这两部分，对。
2: 那么对于一些呃要求比较高的一些数据集，比如说这个 DPO 的数据集。
2: 或者是这个二阶段 SMT 的一些数据集，我们选择了这个人工区域校验。
2: 那么对于 CPD 的数据集的话，我们是通过机器的一些改写。
2: 或者是一些校验，来保证我们 CPU 数据库的一个质量。
2: 对。
2: 那最后一个过程的话是数据的一个质检，然后这个过程的话主要是针对前面的这个数据，然后做相关的一些质量的校验。
1: 对。
2: 接下来是关于高质量数据集的一个定义和要求。
2: 对，这两天这个各种场合都提到高质量数据集，但是呃大都隐隐约约的表达了这个高质量数据集的一些定义，但是没有清晰的给出一个明确的定义。
2: 那么这个是一个找到一个呃论文，包括一些软文里面关于高价值运营的定义，它其实是分了这呃呃六个点，第一个点的话是多样性，对。
2: 第二点是规模大，第三个是合法性，第四个是真实性，第五个是两观性，第六个是没有任何的偏见，并且没有什么无害，对。
2: 呃，这里面很明显的看到就呃规模大和多样性，包括合法性其实是呃优先级比较高的这个要求。
2: 那么有了前面的这个高质量数据定义，包括我们处理的这个流程，那么对于数据收集和说标注这一块，就要进一步的去细化，或者去收集我们这个全要配合大模型的数据了。
2: 右边这个图的话，是我们内部为了做这个大模型，然后造了一个这个标注的平台。
2: 对，然后它可以支支持呃机器和人工去做这个相关的一些数据标注，对。
2: 那么刚才也提到关于数据采集这一块的话，除了刚才提到的开源的这个数据及收集和改写以外，其实我们还做了一个人工，因为社交这个场景其实有一个特性就是他是一个开放域的。
2: 他不是像写代码或者是呃做这个呃做 math 就做数数学题，可能有一个准确的答案，但是涉及到这个场景其实是开放域的，它的难处也在开放域，对。
2: 所以我们后面这个所有的这些方式方法，都是针对开放域的这个问题去提提供的这个解决思路。
2: 对，那么对于弱标注这一块，刚才也提到其实就是更多的规则过滤，无论是一些关键词，包括一些榕树，包括一些恢复了这个生态描写，是不是跟自己的这个内容，生成内容是匹配的，对。
2: 那接下来就关于这个人工标注和质质检这一块，我们举个例子。
2: 对，中间这一块的话是完整的我们训练的一个数据集，呃，然后我们也给出了这个数据的一个要求。
2: 对，红色这个线其实可以看到我们聊天内容的话，其实最终是要满足这个情感的一个呃一个一个一个变化的，或者说我们能够让用户感觉到我们是能够提供情感。
2: 满足了，对。
2: 那满足了这个点的话，其实有两个定义。
2: 第一个是呃你你你的这个情绪有一个正向的变化，比如说原来是你不开心的，现在你通过多种对话以后，你变开心了，这是一个满足点。
2: 第二个点是你如果没有不开心，那么你呃你你你变得更呃没有负面的这个情感表达，也算对。
2: 所以基于这个点的话，我们制定了整个这个数据标注里面的这个这个流程。
2: 从上来以后的这个关心、倾听，到逐步的共情，然后给出建议，再到最后这个开始转移话题。
2: 然后能够找到新新的一些情感的这个宣泄出口。
2: 对，所以这个是一个我们人工标注的一个例子。
2: 那么左边是我们标的过程中需要去关注的一些点。
2: 方便我们在后期筛选数据的时候用到。
2: 对，第一个是关于情感的这个标签，这一块的话就是常规的呃喜怒哀乐中的一些标签。
2: 对，第二块的话是呃聊的内容，对，这也是，就是为了对，造好这个情感陪伴的大模型，我们要更多的去满足不同差异化人群的一个聊天习惯。
2: 所以有了话题的这个这个内容，对。
2: 啊，比如说有些人是聊情感的，有些人是聊游戏的，或者是呃聊音乐的，对。
2: 或有些人是聊美食的，对。
2: 最后一个是呃根据我们定义的内容，然后对这个绘画的质量进行一个打分，对。
2: 这个第一内容的话有，刚才说到了，就是说阳光性、相关性、一致性等各指标，对。
2: 最终给出一个0~5分的一个分值，这个分值是方便我们去筛选出大于3分以上的高质量数据集。
2: 对。
2: 右边这一块的话是刚才说到了几个条件，对。
2: 那在这个过程里面标的时候，我们还关注这个标的内容是否跟这个设计的这个呃 system promo 就指令是不是符合的，对。
2: 还有在情感这个层面上呢，我们还要关注他的一个聊天的简洁程度，对。
2: 还有自然程度，包括流畅，包括内容逻辑或者常识的一些错误，对。
2: 接下来是呃数据比较重的一环，这一环的话是前面有了数据的采集标注，那如何去按我们的模型的迭代目标去采集？
2: 这个数据，那前期前前提就是得有这个数据洞察，对。
2: 呃，我们洞察的方法的话，就前面这三句话，第一个是在呃没有经过人工交互金标的这个数据集上呢，通过大模型，多维度的打分，打标签，然后来指导这个呃模型，这个数据集它的一个标签体系。
2: 第二个点的话是使用大模型对样模进行一个整体的奖励评分。
2: 对，那知道这个标签了，知道分了，那我们就可以知道整个大模型呢，所需要数据集的一个分布。
2: 第三个是去掉这个评分样本比较低的，然后把呃符合我们这个模型迭代目标的数据给采集出来。
2: 这个图的话是举了个例子。
2: 对，比如说我们涉及到关系关系，就关系类别的标签的话，有这个亲密关系、有朋友关系、有亲关系，对，还有行为类别的标签。
2: 还有一些我们涉及到的话题标签，话题这一块的话，有情感交流的，有学习工作的，有日常的，有沟通这个方面的，或者兴趣爱好这个方面的。
2: 对，这里的这个数据和标签都是展示了我们呃 z 时代的一些年轻人常规的一些聊天习惯。
2: 接下来是我们的一个这个模型在实际做了这一步以后，它的一个数据的一个表现。
2: 呃，右边这个表格其实可以清楚的看到我们看那边4.3和4.5的这个。
2: 这个呃数据质量的一个变化，对。
2: 它有多个维度，对。
2: 从聊天深度，感观性，主动性，同理心，然后情感类的一些标签。
2: 然后从4.3然后它的0到1分是刚才说的分值，那么4到5分的话，意味着它的一个数据质量有提升。
2: 对，可以看到4.3的时候它的这个聊天深度只有5.47%但是到4.5的时候已经超过了20%对，那么相对应的他的4.3的时候这个0到1分的占比呃还是有的。
2: 但是到4.5的时候其实已经没有了。
2: 那么一样的这个逻辑。
1: 对。
2: 这个负面程度也是一样的，就是刚好它的这个逻辑是反的，所以说呃负面程度在4.5的时候，它的占比是比较高的。
2: 对这是一个刚才基于我们的这个数据洞察的方法，做了数据筛选以后，可以看到我们不同的训练版本，它这个数据质量的一个变化趋势。
2: 接下来是我们第二帮扶关于模型的一个宣传思路这一块的这个这个设计。
2: 呃，这里面有一个简单的解释。
2: 对。
2: 这边这一块的话，是我们对于 Python train 的一个定义。
2: 对，因为现在很多拿到的大模型都是呃经过 training 以后的一个结结果。
2: 那我们做的这个工作的话，主要是在 Python 这一块，那么呃为了保证我们这个模型是一个社交的，都能对话的这么一个基础模型。
2: 对，所以想做了一个呃 continue p tune 的一个工作，就类似于继续继续训练的一个工作。
2: 呃，这个工作的话主要是呃通过特定的一些领域数据集。
2: 和无标注的一些数据进行一个训练，然后呃输出的主要是一个区域的基础大模型，对。
2: 然后接下来的话我们再去做这个微调。
2: 因为原来我们拿到的这个底膜其实一个通用的大模型，对。
2: 那么经过 CPD 以后其实变成一个吹浴的大模型，然后再做 SFT 和 IIHF 以后变成了一个应用场景的特定大模型。
2: 对。
2: 所以这样的话呢，我们就有了右边的我们这个迭代思路，从这个整个的整体的训练思路，从 CBT 到 SLT 到 IIH，IIHF。
2: 对，这三个阶段。
2: 但是每个阶段解决的问题不一样，然后这个呃右下角这个图呢，是我们最近的这个新的版本。
2: 一个训练的一个流程，对。
2: 它每一个节点上面解决的问题不一样，就是说 c one 这一块的话，呃，CP 这个阶段主要关注的是一个社交的这么一个基础模型的这个语料，所以变成一个社交模型。
2: 对，然后 CG two 这一块的话，主要是强化模型的一个社交聊天，然后这个社交聊天的话是跟我们定义的这个社交聊天的能力和任务有关系的。
2: 对，包括这个情感陪伴、角色扮演这一块的一些能力。
2: 对，那呃第三个阶段的话，主要是呃保留上面设计社交能力的前提下，然后强化他的一些 COT 能力。
2: 对，然后第四个阶段的话，主要是拓展它的一个模型的这个呃长自五创的一个理解。
2: 对。
2: 然后接下来的话是这个强化学习的这个阶段，强化学习这个阶段的话又分为两个阶段，一个阶段的话是强化基础的这个呃能力或者是基础的这个逻辑推理的这个能力，第二部分的话是强化这个社交情感陪伴的这个能力。
2: 对，最后输出我们的完整模型。
2: 目前来看的话，我们呃新的版本话是这个强化学习的第五和第六个阶段可以合到一起。
2: 对，这是一个变化，但是这个是一个早期的一个，量务里面的一个流程。
2: 接下来我们讲一下我们在模型这一块探索出来一些不一样的一些方法。
2: 对，前面各位老师包括昨天也讲到了很多这个大模型。
2: 常规的一些别的方法，对。
2: 那我们在社交这个场景，开放域的这个场景如何去打造一款我们的模型？
2: 然后这一块我们提了一些这个创新点。
2: 对。
2: 第一个点的话是在在训练的过程中，其实我们是选择了用 mask 的一个高效训练的方法，来保证我们的一个训练质量和这个数据集的一个体量能够能够匹配我们去做相关的一些 SLP 的一个训练。
2: 这里面有两个小点，第一个点的话是呃样本里的动态切分，然后保证我们的助手跟回复预诊。
2: 他那个保持一致，对。
2: 因为这里面有一个点就就说他如果回话轮次比较多的时候呢，他就会存在说前面和后面其实聊的内容是不一样的，那通过一个动态切分，能保证我们每一个片段。
2: 每一个片段他们聊的内容其实是跟基础的人设是一致的。
1: 对。
2: 接下来的话第二个点的话是呃细颗粒度的一个 master 的机制，这个主要讲一下，其实就右边这个图啊，对，因为每每每一个绘画组就多轮的聊天里面，其实有很多内容可能是跟绘画的主题。
2: 或者是绘画的内容不是特别相关的，或者有一些还有一些非法的内容，那为了把整个这个绘画的内容组合起来，保证我们的一个绘画质量。
2: 然后保证模型的一个训练量，对。
2: 所以把一些不 OK 的内容做了一个 mask 对。
2: 然后接下来的话是我们常规的一些微调方式，其实我们试过全参数的，呃，包括 Laura 的，包括 k Laura 的。
2: 嗯，然后根据不同的场景我们选择一些不同的这个微调方式，这一块的话就不用展开了。
2: 对，这个太多了，这个电脑。
2: 接下来的话是我们在强调学习这一块的一些点，对，这也是我们区别于别人的一些方法。
2: 这一块的话，有，呃，目前其实我们是走到加 Po 的这个阶段了，对。
2: 然后有两个版本，已经是加 Po 的版本了。
2: 但早期的话，我们其实走过 DPO 的版本，包括 PPO 的一些版本。
1: 对。
2: 目前这个加 PU 的版本的话，呃，主要有两个两个点。
2: 呃，第一个点的话是呃加了一个 Rover model 呃，这是参考了 PPO 的一个一个点。
2: 对，第二个点的话是又又回到 GRPU 里面，关于 Rover Rover function 的这个这个内容，然后去构成了我们 GRPU 的一个奖励函数。
2: 右边是关于这个奖励函数的一个设计，总共有这四个点。
2: 呃，一个的话是刚才说的这个 role model，role model 这一块的话，它更多的是开放域的这个连续性，包括情感的这个衡量。
2: 然后呃这个呃后面的这个奖励函数这一块的话，更多是格式，包括它的一些惩罚项的一些这个设计，对，包括在社交场景里面可能要保证他的一个聊天的字数，对，不能太长，对。
2: 包括还有一个就是他的格式，对，有时候我们要需需要输出，前面是带有一个心理描写，后面是带有这个正常的一个生成内容，所以形成了一个奖励的一个公式，这个公式是呃 Rolling model 里。
2: 呃，然后乘以一个 Alpha 然后加上这个惩罚长度，再乘以一个 beta 然后再加一个这个重复惩罚乘以伽马，然后最后一个格式惩罚乘以西格玛。
2: 对。
2: 那我们现在这个实际训练完以后测试的一个结果来看的话，呃，就如果是 model 这一块的一个这个权重是最大的。
2: 对，然后依次大于这个呃长度惩罚，再大于这个重复，再大于这个格式惩罚。
2: 对，那么根据目标其实可以去调整的，这个阿尔法和贝塔。
2: 接下来是关于情感评测这一块的一个内容。
2: 对，这块也是跟高校合作的一个一个结果。
2: 对，然后这个参考了这个呃上海的 AI lab 的这个 open Compus。
2: 包括 super call 的一些这个评测的框架，然后设计了这个呃区域的一个评价方法，然后总共有这三块。
2: 第一块的话是呃这，从一级来看的话是专业与技能，呃，然后中间一块的话是预言与知识，然后最后一块是模型安全，因为我们是一个一个情感陪伴的，所以把情感陪伴相关的这个内容做了加加深的一个处理。
2: 对，包包含情感陪伴，包含角色扮演。
2: 但其他的能力的话，只是一个浅色的。
2: 这是一个具备，包括这个初级计算，包括逻辑推理，包括多轮，对。
2: 那么到第三级这一块的话，对于情感陪伴这一块做了六大任务的一个拆解。
2: 包括心理安慰，包括谈情说爱，包括不了倾听等。
2: 然后再往下的话，又会涉及到更细的一些拆解，比如说呃这个谈情说爱这一块可能更多的是爱情，然后对于这个心理或者是无聊这一块的话，更多的是一些日常的关系。
2: 对，那么对于角色扮演这一块的话，会涉及到动漫人物。
2: 对，包括还有一些这个呃因为这个呃 z 时代的年轻人对学业和经济这一块的这个压力比较大，所以这一块的话也涉及到相关的一些问题。
2: 对，呃，以上是整个这个评测的一个整体框架。
2: 再来的话是这个，据刚才那个框架，去做相关的一些评测，对。
2: 然后这个评测的话分为两部分，第一部分的话是左侧的这个客观的部分。
2: 第二部分的话是右侧的这个主观部分，然后客观这一部分的话又分为了这两个点，一个是智商的部分，呃，另一个点的话是情商的部分，对。
2: 智商这一块的话也是参考刚才说到的这个，业业界的一些开源的这个通用的能力评测，对。
2: 那情商这一块的话就是我们整个这个框架里面的核心部分。
2: 主要有分为三个点，第一个点的话是气象两表这一块的一个测试。
2: 第二个点的话是情感子任务的一个测试，第三个点的话是呃情感满足度的一个测试，对。
2: 那么有了这这这几块的一个测试，其实我们能够呃轻易知道我们这个模型在智商和情商这一块的一个得分。
2: 对，右边这一块的话，是我们在呃日常体验和测试过程中的一个约束，也给了这个量化的一些呃约束条件。
2: 对。
2: 比如说，对于对于 SPS 就清单满足度的这么一个定义，那我们给了这个呃五个档位，对。
2: 那么中间的四档，就4到6分的这个档位的话，是一个呃常规的一个中间的一个档位。
2: 那么越往越往右，那这个呃情感满足会越高，越越越往左，它那个得分越低。
2: 这是模型评测这一块的一个一个内容，这也也说一下，就这个，后面我们会把呃相关评测的这个内容，包括数据集会做开源。
2: 然后这个开发工作应该是在8月份应该会会会发布。
2: 接下来是我们对这个模型在刚才说到了这个客观和主观评测上的一个结果。
2: 对，这个左边这个图。
2: 是不同的厂家，对对不同的评测数据进行上的一个得分。
2: 那我们可以看到对，希望他开店的一个动工情况。
2: 对，他的呃两个数据集上那个得分，最高的有79.4的分，啊，第二个数据集是77.65分。
1: 对。
2: 呃，右边这个图的话，是我们做了拆解以后关于主观和客观这一块综合的一个呃评价指标，对。
2: 包括包括那个刚才说到了这个清单量表和清单任务，对。
2: 总共有18个维度，我们可以看到其中我们有12个维度是领先于其他的一些模型的。
2: 再来最后一一点，关于模型的一个部署和体验。
2: 对，因为前面说到了这个模型的评测，那最后一步就是模型的这个不是，这块的话其实有公司自研的一个模型的，这个虚拟推的这么一个平台，然后帮助我们去做模型部署。
2: 以上是第三部分内容，第四部分内容是关于模型应用和未来的一个展望。
2: 这一块的话有有两个点我们在探索中。
2: 第一个点的话是就做这个呃 AI 身高这一块的一个情感陪伴的探索。
2: 对，然后两个需求的场景，一个是承接新用户的场景。
2: 然后第一个，另另一个是成交老用户的场景。
2: 第二个业，这个业务探索的场景的话，主要是来自于教教育回复这一块，对。
2: 然后这也是我们在那个工作流的平台。
2: 然后这里面可以看到我们有不同的这个分支，然后选择不同的模型。
2: 这里面其中有三个分支是选择了开源的模型。
1: 对。
2: 接下来是开店模型的一些掌握，对。
2: 然后这掌握的话要分成两部分的内容，第一部分内容的话主要是这个模型的核心能力的一个掌握。
2: 这一块的话包括情感咨询，包括托尼倾听，包括现在业务题的一些恋爱拉扯的能力的一个增强。
2: 对，这一块是我们这个未来要去大力呃发力的一个点。
2: 第二块的话是技术这一块的一个突破点，呃，这一块的话有分为三个小点，呃，第一个点的话是优化模型。
2: 对算法，尤其是 MOE 架构下面的一个算法，然后提升模型的一个推理速度。
2: 对。
2: 第二点的话是刚才也说到就开放率的加 PL 的一个奖励函数的设计。
2: 这一块的话我们跟高效的做合作，然后在这一块也希望能够呃有一些成果。
2: 第三个点的话是动态 cop 的一个能力，能够自主的去可控然后用户可以无感的去选择呃使用推理或者不使用推理。
1: 对。
2: 那右侧这个图的话，是这个整个呃模型这一块未来的一个方向，就是能够做成一个呃心灵陪伴的这么一个产品。
2: 接下来是关于这个模型类应用侧的一些展望。
2: 呃，这一块的话也有这四个小点。
2: 呃，第一个小点的话是刚才前面就说到对，呃，我们整个的这一块的这个情感陪伴大模型有两个方向，一个是硬件，智能硬件的这个陪伴产品，一个是呃应用侧的一个陪伴产品。
2: 对，那我们在这一块的这个呃趋势，也更多的是围绕呃比如说儿童类的一个平板平板产品，然后去打造智能硬件。
2: 第二块的话是应用这一块，那应用这一块的话也也有这三个点。
2: 第一个的话是有温度的智能客服，第二个点的话是你的话的一个陪练。
2: 第三点的话是呃情感咨询的一些助手，对。
2: 然后这一块的话能够通过开箱加业务场景的一些小规模的微调，然后打造这个不同场景的一些落地。
2: 对，呃，尤其我们现在在有温度智能客服这一块其实做了一些探索，并且呃在一些业务场景上获得认可。
1: 对。
2: 这是参加的一个应用找光，对。
2: 这样来的话是最后一个关于我们开元全量标大模型的一个总结。
2: 对，这会的总结其实跟我们前面提到这个问题是一致的。
1: 对。
2: 呃，刚才也提到对，z 时代的用户其实是一个孤独，然后又又追求躺平、快乐、社交，然后找搭子的这么一个群体。
2: 对，那我们这个模型的一个目标，或者公司的目标，更多的是用 AI 去解决用户的这个孤独，然后提供情感陪伴的一个需要。
2: 对以上是我今天分享的内容，然后，也感谢大家一个聆听。
1: 请问来宾们有没有什么对子问？
3: 老师你好，关于刚才提到的 GRPO 训练那里，呃，我有个问题想了解一下，就是您提到了有四项的奖励，就分别是那个奖励模型、长度惩罚、重复惩罚以及格式惩罚。
3: 那呃我训了一版模型，就是怎么去调节这个超参数，使得后续的这个模型的效果更优。
3: 你们因为因为这个奖励里面包含了四个超参数。
3: 可能对于 GRPO 来说，在个人任务里面这算是一个比较复杂的。
3: 那呃我举个例子，就比方说我训练完一版模型，我分析 decades 然后发现重复的这个问题特别严重，那么我就要去把这个重复惩罚的这个超参数调大，那么我同时要满足这个重要性的排序。
3: 因为重复它是排在你刚才提到的第三重要性，我同时要把第二重要性跟第一重要性的那个超参数调高，那么怎么去调高？
3: 这个你们有没有一些特定的方法呢？
3: 还是说我人工靠感觉，然后把这个数值稍微调大，它就需要改这样。
2: OK 我我我解析下这个问题，主要是说如何在加 PV 里面去调整它的一个全系数，保证在每一个结果出来以后符合预期。
2: 然后我回答一下第一个点，第一个点的话，呃，其实没有什么好的办法，我们也是不断的在测试的过程中发现，比如说我们重复的次数过高了，那我们把这个重复的这个奖励函数。
2: 然后惩罚放大一点，然后都保证它每一次出来的时候符合我们这个要求不应该重复，对。
2: 然后呢这个调的过程呢，其实是一个不断探索的过程。
2: 原来我们这个重重复这个系数应该是在0.15然后调到1.17对，调完以后又发现这个他的差异化又小了，那我们再回去，就不断的这么一个摸索的过程。
2: 但是还有一个地地方呢，就是我我想讲一下，其实在做加铺之前我们是做了这个三阶段的 SFT 那么如果是出现大量重复的这个结这个结果的时候，我们修的不应该是在强化学习这个阶段。
2: 应该考虑的是上一个阶段。
2: 对，是 SFT 这个阶段，对。
2: 所以后面我们也发现就是有些问题在强化学习这个阶段其实是解决不了的。
2: 呃，那你本身 SFT 这个阶段已经出问题了，你通过强化学习其实拉不回来，那我们就在 SFT 这个阶段去 check 它的一个数据质量，然后去保证在 SMT 这一块的话，它就不出现大面积的重复，那么在呃强化学习这个阶段，它的效果就会出来。好的，谢谢老师。好
1: 的，谢谢。
4: 哎，老师你好，就是，我也想呃就是请教一下，就是说你前面说的那个 GRPO 那个奖励函数的那一块，呃呃，GRPO 的话，因为我我理解就是说它是一个很高效的一个强化学习算法。
4: 对，然后然后就是说，但是我刚看你上面写的，就是有一个关键的是奖励模型，我想了解了解一下，就是说这个奖励模型它是一个模型啊，还是一个，就是说就是说我自己用训练数据训练起来一个模型。
4: 还是说一个函数，可以好秒就是计算出来一个一个这么一个函数。
4: 这这个。
2: 呃，我要回答一下这个结论，这个是一个呃 model 是一个模型，是一个呃是一个小参数量的一个评价模型。
2: 对，然后我说一下这个问题啊，就是这个问题其实是说，呃呃，像 GPS 的时候，包括 TOMCAT3的时候，关于加票这一块呢，更多是一些规则，或者是说奖励函数。
2: 对，这一块的话，它的计算基本上是在毫秒级别就可以跑完。
2: 他说我们的这个问题是，上来就说了我们这个解决问题是一个开放域的，就没有说社交场景下面哪一个问题是准确的，比如说我失恋了应该怎么办？
2: 你你你这没有说哪个是准确答案，必须是你失恋了就如何如何吧。
2: 对，所以在这个层层面上面，我们解决的问题就必须是一个通过模型综合去判断的，它是依照上下文选择一个局部最优。
2: 局部最优的一个结果，所以这个局部最优的这个选择方式，就通过如果的 model 去解决。
1: 对。
4: 另外还再追问一下，就是说呃您觉得就是 GRPO 算法跟呃是用 GRPO 去训练进行后阶段的训练好，还是说用结结，用那个 d 二 P O 结合 Lora 的这种一个形式去训练比较好一点？
2: 呃，现在我我说一下我们的这个情况，我们早期是 DPO 的情况，然后这后面就 PPO 你看现在其实是一个 JRPO 的一个一个一个改写版。
2: 其实用了这个 PPO 的 reward model 加了这个 JRPO 的奖励函数组成的这么一个 JRPO 的方式，对。
2: 然后这个过程里面其实更多的时候考虑到你面临这个问题，是不是需要用准确的这个奖励函数去解决。
2: 如果奖励函数能够解决，其实就不需要用 reward model 去了。
2: 包括刚才你提到了，用不用，要要加一个什么 laws 去做这个约束？
2: 主要看前面这个问题是不是需要开放域里面有一个模型去解决。
1: 对。
2: 没有，我我理解可能没有一个准确的答案，包括我们现在这个也不是一个，我我理解也不是一个最终版。
2: 对。
4: 老师你好，感谢您的分享。
4: 就是我有几个问题可能想咨询一下。
4: 第一个就是那个，呃，我看您那个训练数据的那个构造里面，那个那个标注的那个标签的力度非常细致嘛。
4: 嗯，是，对，所以我觉得可能这是一个非常大的一个一个耗费，非常多的那个成本，想问一下您这一块，就是比如说从预训练到就是继续训练到那个 SFT 到强化学习，整个我们的那个训练数据的。
4: 分别的训练数据的规模大概是一个什么样的，以及成本大概是什么样的。
2: 对，我想回答第一个问题，第一个点就是其实我们最早不是这样的，是踩过坑以后再变细的。
2: 对，是踩过坑以后再联系，就每一次测完以后，我们发现我们的某些指标下掉了，才发现我们在某个数据集上面没有这一块的标签，不足以支撑我们去做归因分析，所以我们才仿工把整个这个数据标签体系完善起来。
2: 所以现在展示的是结果，其实过程迭代了很多版，对，这第一个点，第二点的话就是这个数据体量呃是这样的，就是呃跟场景有关系吧，对，我我我说一下我们社交这个场景，社交这个场景里面。
2: 因为我们的底模其实是一个通用模型，那么基本没有什么社交的这个，多种对话的能力。
2: 所以在 CPT 这个阶段的话，大概的这个数据量是百万级别的。
2: 然后 SMT 这个节级别，级级别的话是，s SMT 这个阶段的话大概是在10万级别。
2: 然后，Rolling model 这一块呢，是不到万级别的一个一个数据体量。
4: 好的好的，谢谢。
4: 然后另外一个点就是，你们前面提到有引入那个一些心理学的那个那个知识嘛，我看是不是主要是在那个，就是训练数据以及那个最后评估这地方是会加入。
2: 呃，有有三个地方，第一个就是刚才数据，数据的这个改写和生成，这个里面需要有很多这个心理学的知识，因为刚才一直在强调这个社交场景，不像其他的什么高顶或者是数学的场景。
2: 所以这个场景里面有很多这个心理学的知识要注入，帮助我们去提升在数据生成的这个质量，这第一个节点，第二个节点就是我们的模型出来以后怎么去评价。
2: 现在跟心理学的一些专家，包括业界的一些高校合作的点，主要就是这个如何去评价这个模型，因为你是一个自研的这个模型，对，那你得有一套这个自研场景下面，对模型的这个客观评价。
2: 对，所以第二个点是评价，第三个点是用心理学的这些知识去做业务场景的一个引导，就比如说我在训练的时候，我在评价的时候评价了模型的谈情说爱的能力，那么业务侧应该在上线的时候要关注一下用户是否有谈情说爱的诉求。
2: 大概是这么一个逻辑，明白，好的，谢谢。
4: 然后还有一个点就是呃刚有一个地方，那个会是那个多个 SFT 那个集连的猫，OK 然后这个地方我很奇怪，就是为什么不是把这几块的数据混在一起，然后整体做一个 SFT 而是对。
2: 这个也也还是踩坑的一个结果，就我们原来就是混到一起的，一个 SFT 解决所有的问题，但是发现以后解决不了，他互就互相是干扰。
2: 比如说我在 SFT 阶段，既要解决情感陪伴的核心能力有所提升，又要保证他 cut 的能力不能下降。
2: 这两个节，就这两个目标在一个 SMT 里面很难去达成统一，对，所以我们才拆成不同的阶段，让他去强化单一的能力，这样的话效果更好一点。
4: 那就是训练数据也是完全切开的，是切开的。
4: 好的好的，谢谢。
4: 然后然后我再，因为时间有点长，我再最后再问一个问题就是就是您最后有提到衍生的，就是有想去做一些有温度的智能客服，啊，OK 然后这种传统的智能客服其实更多是要去解决用户的问题吧。
4: 因为用户有直接的诉求，然后这个有温度的话，其实要解决的是带有一些陪伴的这种这种这种，所以这个地方是如何？
4: 就是我们具体在实施的时候怎么去怎么去融合这种解决问题跟陪伴这两个不同的这个呃关系？
2: 对，这个也是我们其实嗯最早在我们客服这个场景考虑的一个点，就是其实这种客服的这个根本其实解决问题，这个没有错的，对，所以我们定义这个本就是解决问题，解决问题这个主流程不应该是在开江承接的。
2: 然后如果用户有出现情绪方面波动的，比如说你打了10086，对，这个问题还没解决，然后下一句你可能就发散起来了，对吧？
2: 然后在发散的这个过程，把这一部分的这个你需要去被安慰的这个点，转到开店。
2: 其实它前面有一个在在这个呃智能客服里面，其实有一个意图分类的这么一个过程，就是根据你用户的这个情感变化，或者是你要解决的问题。
2: 导流到不同的模型，对。
2: 我们有温度的人讲，更多的是用户在问题没有解决以后出现的问题，比如有情感陪伴的这个模型，去帮你做相关的想要的和缓解。好的，谢谢谢谢。
