# 基础知识

本文用于积累模型推理加速相关的技术文档。从具体的技术思路来看，推理加速有很多不同的实现方式，比如量化、剪枝、蒸馏、并行等等。

## 评估指标

如何评估一个推理框架/LLM的性能？
[评测指标](./评测指标.md)

一般推理框架在其项目目录中都会包括Benchmark脚本，直接运行就能评估各项指标。普遍的思路是这样的：先启动一个LLM服务端，再用客户端基于dataset连续发出请求，根据响应的时延等信息来评估指标。
有时需要额外对后端代码中的http请求体、dataset的格式进行手动处理，来适配不同的评估场景。

## 分布式推理与并行策略

[并行策略简介](./concurrency_intro.md)

[Distributed Inference and Serving](https://docs.vllm.ai/en/latest/serving/distributed_serving.html#running-vllm-on-multiple-nodes)

[vLLM PD分离方案浅析](https://zhuanlan.zhihu.com/p/1889243870430201414)

## 模型加速

[大模型蒸馏](https://www.zhihu.com/question/625415893/answer/3243565375)
