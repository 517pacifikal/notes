# 业务类模型推理/部署优化方案

本文档聚焦于提供偏向于业务的开箱即用大模型推理优化方案介绍

## 一、常规参数量模型部署方案

1. 移动端部署
    
   高通QIDK、Apple CoreML、腾讯NCNN、阿里MNN、各类魔改tflite/onnxruntime-edge等库

2. 边端部署

   各类基于onnxruntime-edge/TF-Edge/libTorch的开源SDK

3. 部署小参数量模型+追求较低E2E-Latency
   
    nv Ampere前的老卡(建议走torch-compile+uvicorn,小参数量模型效果较好)

4. 异构部署方案推荐
   
    ascend/mlu/amd等硬件厂商自己提供有部署SDK ; 追求可迁移建议使用 TVM

## 二、大参数量模型

此领域发展日新月异， 下述调研内容也仅重点聚焦于本组主要面向的业务场景，故此文档中的内容仅供参考。

### 1 框架层面

#### 1.1 高性价比部署路线

目前业界各类LLM推理部署框架非常成熟，图简单低成本部署可专注于VLLM、SGlang(目前业界首选的两个框架，前者对于各类模型、后训练推理优化方案支持得比Sglang更好一点，单Node分布式推理两者都较为简单，多Node分布式推理Sglang部署较为简单，其自己实现了NCCL通信逻辑。VLLM需要结合Ray,更麻烦一点，但服务部署起来之后比SGlang更稳健一些，因通信问题服务崩的概率更小。同时对于MOE架构的模型，目前来说使用SGlang是更为主流的方案，性能大概率比VLLM要强，但需要仔细挖掘sgl官网对于各类moe超参的使用描述)

LMDeploy(对于各类国产多模态模型支持得较好，使用起来跟前两者一样简单，部署多模态模型可能更方便一些)

#### 1.2 追求更高程度的模型服务性能优化路线

TensorRT-LLM+TritonServer (Nvidia生态有各种极高性能的推理优化方案，但其中部分实现对开源框架的兼容度不够，主要面向他们自家trtllm、megatron-lm等，如果NVFP4、sparseGPT等后训练推理优化方案。对于业界主流的开源模型: Qwen、Deepseek、Llama等，使用trtllm部署的难度会小很多，如果是非主流开源模型，例如是单纯的torch-api定义的小参数量模型或推理行为较为特殊的模型，不建议使用trtllm，部署成本较高，短期内去尝试性能未必有明显提升，甚至很可能因为各类因素性能较低)

FasterTransformer：追求极致非主流开源&小参数量Transformer基座模型的单Batch推理时延优化可尝试此框架，虽然Nvidia官方目前声明已经讲FasterTransformer标记为deprecated,建议开发者迁移至trtllm，但上述描述的场景下或许此框架的性能要强于trtllm(FT无Iterative batch机制)，trtllm更适合优化开源大模型的全局吞吐

Nvidia Dynamo+TritonServer: 追求超高性能、大规模参数模型模型服务(如DS-V3)，应用于MAAS、PAAS场景，开发成本较高但属于目前相关应用场景最前沿主流的部署方案之一

RTP-LLM: 如果尝试使用V100等老卡进行模型服务部署,特别是Qwen架构的模型，可以尝试使用阿里自家研发的RTP-LLM，针对V100做了定制化cutlass kernel优化，特别针对小参数量模型，此框架优势会更加明显(基本由C++编写，框架本身的launch overhead相对VLLM、Sglang较小)

#### 1.3 异构部署方案
##### 1.3.1 纯CPU

llama.cpp

##### 1.3.2 NV GPU & CPU Offload

GPU占比较多推荐VLLM、SGlang，目前两者都支持cpu-offload,但vllm支持offload的模型种类相对多一点 
CPU占比较多推荐KTransformers、llama.cpp
1.3.3 Ascend

目前torch-npu对ascend对支持已经逐渐超越AMD GPU，可直接使用vllm-ascend；以及推荐其自家的MindSpore、各类基于CANN的开源SDK
 1.3.4 其他主流异构硬件产品部署
MLC-LLM: 基于TVM，陈天奇团队转为LLM推理开发的异构部署框架

### 2.后训练推理优化方案(完全不涉及修改模型实现版)

各类大模型量化方案，目前方案非常多，可自行阅读相关论文以及在开源论坛了解各类量化实现的优缺点对比，这里主要聚焦于描述各类框架对于它们的兼容性。目前来说VLLM对各类量化库实现的兼容性明显强于SGlang，基于如今部署大语言模型使用的GPU基本上架构为Ada、Hopper及以后，追求尽可能地不影响模型生成结果准确性首推FP8，追求较高性能可尝试w8a8、NVFP4等，追求更高性能时可进一步考虑稀疏化，目前主要聚焦于主流的2:4半结构稀疏、sparseGPT(2:4、4:8、unstructure)。下面是目前主流开源量化库介绍:

sglang、vllm: 两者均支持在线FP8量化，目前sglang实现了per_tensor、per_token的online-fp8量化，vllm似乎只实现了per_token的online-fp8量化，若使用两者的在线量化时遭遇了精度或性能相关的问题，可尝试仔细研究此方面，量化算法的不同实现会形成两者不同的trade-off。同时目前vllm对于各类开源量化库的量化模型权重格式支持度也强于sglang，可自行去官网查看

bitsandbytes: 目前bnb主要支持各类int类型量化(重点基于SmoothQuant)，支持unified huggingface格式权重的推理框架应该就支持部署其输出的权重

torchao: 支持丰富的各类训练/后训练量化方案，属于较底层的库，一般的大模型推理优化库会集成torchao, 若需要对开源主流大模型进行量化,直接使用torchao可能会更为繁琐，建议使用更上层的量化库

llmcompressor: 量化方面基于transformerEngine、torchao等底层量化库，稀疏化方面基于torchao，目前比较成熟的优化+部署路线是 llmcompressor/compressed_tensor → vllm(目前已验证了fp8量化、2:4稀疏化使用此方案较为简单),sglang对llmcompressor的支持不好，目前官方社区正在更多地转向支持modelopt,同时此库中还集成了剪枝微调。

TransformerEngine: 主要实现的是Transformer基座模型的fp8量化方案，包括qat/ptq,llmcompressor的fp8 ptq底层就是基于的TE库

ModelOPT: 支持各类大模型(LLM、VLM、Diffusion、Speculative-Decoding模型)、各类qat/ptq量化方案(fp8、smoothquant、w4a16_awq、w4a8_awq、nvfp4等)，同时支持sparseGPT(集中于2:4半结构稀疏，对于4:8、unstructure sparse官方文档没有过多描述)。但目前modelopt兼容度最好的框架是其自家的trt、megatron等，其量化权重格式可通过脚本转化为Unified-huggingface格式权重来部署至其他主流框架，训练层面兼容度不够。其稀疏化权重目前不太兼容其他主流部署库

llama.cpp:  GGUF格式的量化格式目前已被绝大多数主流框架所兼容
 
### 3.服务层面优化
#### 3.1 单卡部署多模型服务实例

建议torch-compile + kui/uvicorn

#### 3.2 多Node、多卡部署多服务的高性能请求分发

  一般来说目前主流框架都有自带的路由(自己开发请求分发路由成本较高，Cache-Aware机制不好处理)，基于Cache-Aware、Load-Balancing等机制深度优化，例如SGlang的sglang-router、VLLM结合Ray Server或直接应用vllm官方团队的K8s-Native inference system开源项目: production-stack、TensorRT结合TritonServer等

#### 3.3 合理利用各种开源库，分析模型推理行为以进行定制化优化  (一般是针对小参数量的torch-based模型脚本做优化)

  例: 1.针对某些输入shape固定的模块可以预先基于torch.jit做cache(后续torch会支持基于dynamo的Mega-Cache,与torch-compile兼容度更好)，将可以分理出动态图的部分做静态cache，后续直接对接torch-compile包裹出的动态图，有时可以提高整体推理性能  2. 对于基于torch-api定义的类Transformer-Based模型，可以使用triton编写融合算子以替代原来的各个分离算子或直接基于flashinfer替换原脚本中的对应运算部分(目前支持custom/common的prefill、decode、rope、rmsnorm等常见高性能算子，且与torch-dynamo高度兼容)，profiling可使用torch-profiler
